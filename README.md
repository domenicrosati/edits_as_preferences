## Edits as Preferences ##

Experiments with using preference learning for model editing.

ToDo:
- [ ] Add full counterfact_sample for comparison
- [ ] Add reference model variations
- [ ] Do hyperparam sweeps for DPO
- [ ] Implement other DPO variations (iPO, cPO, etc.)
- [ ] Implement other policy gradient methods (PPO, TRPO, etc.) to compare
- [ ] Experiment with ways of expanding preference samples
- [ ] Experiment with different reward functions beyond DPO 
- [ ] Experiment with batch and chain setting 
- [ ] Experiment with sample efficiency across expanded preference samples 
